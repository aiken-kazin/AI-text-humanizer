{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from humanizer_chain import humanize_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"Biology is the scientific study of life and living organisms, encompassing subdisciplines that examine the structure, \n",
    "function, growth, origin, and evolution. Molecular biology focuses on interactions among cellular components such as DNA, RNA,\n",
    " proteins, and other biomolecules. Cellular biology delves into structure and function of cells, units of life. Within cells, \n",
    " organelles like mitochondria, chloroplasts, and the nucleus illustrate specialization for energy production, photosynthesis, \n",
    " and genetic regulation. Processes like mitosis and meiosis are essential for growth, \n",
    "reproduction, and inheritance, while cell signaling enables multicellular coordination.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biology is all about exploring life and the living world, diving into areas that look at how things are built, how they work, how they grow, where they come from, and how they've changed over time. Molecular biology zooms in on the interactions between the tiny parts of cells like DNA, RNA, proteins, and other important molecules. Then there's cellular biology, which takes a closer look at cells themselves â€” the building blocks of life. Inside these cells are organelles like mitochondria for energy production, chloroplasts for photosynthesis, and the nucleus for managing genetic information. Processes such as mitosis and meiosis play crucial roles in helping organisms grow, reproduce, and pass on traits to their offspring. And thanks to cell signaling, cells in multicellular organisms can communicate and coordinate with each other effectively.\n"
     ]
    }
   ],
   "source": [
    "humanized_text =  humanize_text(text)\n",
    "print(humanized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/miniconda3/envs/demo/lib/python3.10/site-packages (4.47.0)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/envs/demo/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /opt/miniconda3/envs/demo/lib/python3.10/site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/miniconda3/envs/demo/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/envs/demo/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/miniconda3/envs/demo/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/miniconda3/envs/demo/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/miniconda3/envs/demo/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/miniconda3/envs/demo/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/miniconda3/envs/demo/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/miniconda3/envs/demo/lib/python3.10/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/miniconda3/envs/demo/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/miniconda3/envs/demo/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/envs/demo/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/envs/demo/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/envs/demo/lib/python3.10/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/envs/demo/lib/python3.10/site-packages (from requests->transformers) (2025.4.26)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
      "    Setting a new token will erase the existing one.\n",
      "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Enter your token (input will not be visible): Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/envs/demo/bin/huggingface-cli\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/opt/miniconda3/envs/demo/lib/python3.10/site-packages/huggingface_hub/commands/huggingface_cli.py\", line 59, in main\n",
      "    service.run()\n",
      "  File \"/opt/miniconda3/envs/demo/lib/python3.10/site-packages/huggingface_hub/commands/user.py\", line 111, in run\n",
      "    login(\n",
      "  File \"/opt/miniconda3/envs/demo/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py\", line 101, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/miniconda3/envs/demo/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py\", line 31, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/miniconda3/envs/demo/lib/python3.10/site-packages/huggingface_hub/_login.py\", line 130, in login\n",
      "    interpreter_login(new_session=new_session)\n",
      "  File \"/opt/miniconda3/envs/demo/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py\", line 101, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/miniconda3/envs/demo/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py\", line 31, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/miniconda3/envs/demo/lib/python3.10/site-packages/huggingface_hub/_login.py\", line 287, in interpreter_login\n",
      "    token = getpass(\"Enter your token (input will not be visible): \")\n",
      "  File \"/opt/miniconda3/envs/demo/lib/python3.10/getpass.py\", line 77, in unix_getpass\n",
      "    passwd = _raw_input(prompt, stream, input=input)\n",
      "  File \"/opt/miniconda3/envs/demo/lib/python3.10/getpass.py\", line 146, in _raw_input\n",
      "    line = input.readline()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here is a rewritten version of the text passage in a style that is typical of an IELTS band level 6 writer:\\n\\nResearch into fractal geometry is becoming increasingly important across various fields. For example, algorithms based on fractals are used to improve image compression and texture synthesis. In medicine, fractal analysis can be used to understand irregular heartbeats or detect patterns in tumor growth. The way that fractal geometry interacts with other branches of mathematics such as measure theory, probability, and number theory is also being studied.\\n\\nNote: I made the following changes:\\n\\n* Simplified vocabulary (e.g., \"entirely essential\" instead of \"essential role\")\\n* Used more concrete examples to illustrate the concept\\n* Changed some sentence structures to improve clarity and coherence\\n* Made minor grammatical adjustments (e.g., \"improving image compression, texture synthesis, and pattern recognition\" is a bit wordy, so I changed it to \"using algorithms based on fractals to improve these tasks\")\\n* Removed some of the more complex phrases and idioms (e.g., \"interactive and fruitful area of study\")'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in .env file.\")\n",
    "\n",
    "\n",
    "llm = OllamaLLM(\n",
    "        model='llama3.2:1b'\n",
    "    )\n",
    "\n",
    "\n",
    "default_prompt = \"\"\"\n",
    "Rewrite the following text passage to reflect the writing style of \n",
    "a non-native English speaker who has achieved a band level 6 in IELTS writing. \n",
    "This level indicates a competent user of English, but with some inaccuracies, \n",
    "inappropriate usage, and misunderstandings. \n",
    "The text should be mostly clear but may contain occasional errors in grammar, vocabulary, and coherence.#\n",
    "\n",
    "Text Passage for Rewriting: {input_text}\n",
    "\n",
    "Note: Aim for errors that are typical of an IELTS band level 6 writer. \n",
    "These could include minor grammatical mistakes, slight misuse of vocabulary, \n",
    "and occasional awkward phrasing. However, the overall meaning of the text \n",
    "should remain clear and understandable.# Word Count: approximately 400#\n",
    "\n",
    "In your answer please avoid the text like 'Here is a rewritten version of the ...'\n",
    "\n",
    "Humanized Version:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "text = \"\"\"In contemporary research, fractal geometry plays an essential role in fields ranging from physics and computer graphics to finance and medicine. For instance, fractal-based algorithms improve image compression, texture synthesis, and pattern recognition. In medicine, fractal analysis assists in characterizing irregular heartbeats or tumor growth patterns. The interplay between fractal geometry and other branches of mathematics, such as measure theory, probability, and number theory, continues to be an active and fruitful area of study.\"\"\"\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=default_prompt)\n",
    "\n",
    "humanizer_chain = prompt | llm\n",
    "response = humanizer_chain.invoke({\"input_text\": text})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"In contemporary research, fractal geometry plays an essential role in fields ranging from physics and computer graphics to finance and medicine. For instance, fractal-based algorithms improve image compression, texture synthesis, and pattern recognition. In medicine, fractal analysis assists in characterizing irregular heartbeats or tumor growth patterns. The interplay between fractal geometry and other branches of mathematics, such as measure theory, probability, and number theory, continues to be an active and fruitful area of study.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResponseError",
     "evalue": "model 'llama3.2' not found (status code: 404)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResponseError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/Users/ajkenkazin/ALL-PROJECTS/AI_Projects/AI-text-humanizer/app/chains/test.ipynb Cell 9\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ajkenkazin/ALL-PROJECTS/AI_Projects/AI-text-humanizer/app/chains/test.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m prompt \u001b[39m=\u001b[39m ChatPromptTemplate\u001b[39m.\u001b[39mfrom_template(template\u001b[39m=\u001b[39mdefault_prompt)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ajkenkazin/ALL-PROJECTS/AI_Projects/AI-text-humanizer/app/chains/test.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m humanizer_chain \u001b[39m=\u001b[39m prompt \u001b[39m|\u001b[39m llm\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ajkenkazin/ALL-PROJECTS/AI_Projects/AI-text-humanizer/app/chains/test.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m response \u001b[39m=\u001b[39m humanizer_chain\u001b[39m.\u001b[39;49minvoke({\u001b[39m\"\u001b[39;49m\u001b[39minput_text\u001b[39;49m\u001b[39m\"\u001b[39;49m: text})\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ajkenkazin/ALL-PROJECTS/AI_Projects/AI-text-humanizer/app/chains/test.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m response\n",
      "File \u001b[0;32m/opt/miniconda3/envs/demo/lib/python3.10/site-packages/langchain_core/runnables/base.py:3047\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3045\u001b[0m                 input_ \u001b[39m=\u001b[39m context\u001b[39m.\u001b[39mrun(step\u001b[39m.\u001b[39minvoke, input_, config, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   3046\u001b[0m             \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 3047\u001b[0m                 input_ \u001b[39m=\u001b[39m context\u001b[39m.\u001b[39;49mrun(step\u001b[39m.\u001b[39;49minvoke, input_, config)\n\u001b[1;32m   3048\u001b[0m \u001b[39m# finish the root run\u001b[39;00m\n\u001b[1;32m   3049\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/demo/lib/python3.10/site-packages/langchain_core/language_models/llms.py:389\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[39m@override\u001b[39m\n\u001b[1;32m    379\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\n\u001b[1;32m    380\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    386\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m    387\u001b[0m     config \u001b[39m=\u001b[39m ensure_config(config)\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 389\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_prompt(\n\u001b[1;32m    390\u001b[0m             [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_convert_input(\u001b[39minput\u001b[39;49m)],\n\u001b[1;32m    391\u001b[0m             stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    392\u001b[0m             callbacks\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcallbacks\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    393\u001b[0m             tags\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mtags\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    394\u001b[0m             metadata\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmetadata\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    395\u001b[0m             run_name\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mrun_name\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    396\u001b[0m             run_id\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mpop(\u001b[39m\"\u001b[39;49m\u001b[39mrun_id\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    397\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    398\u001b[0m         )\n\u001b[1;32m    399\u001b[0m         \u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m    400\u001b[0m         \u001b[39m.\u001b[39mtext\n\u001b[1;32m    401\u001b[0m     )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/demo/lib/python3.10/site-packages/langchain_core/language_models/llms.py:766\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[39m@override\u001b[39m\n\u001b[1;32m    758\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_prompt\u001b[39m(\n\u001b[1;32m    759\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    763\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    764\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    765\u001b[0m     prompt_strings \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_string() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[0;32m--> 766\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(prompt_strings, stop\u001b[39m=\u001b[39;49mstop, callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/demo/lib/python3.10/site-packages/langchain_core/language_models/llms.py:973\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m get_llm_cache() \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    959\u001b[0m     run_managers \u001b[39m=\u001b[39m [\n\u001b[1;32m    960\u001b[0m         callback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[1;32m    961\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_serialized,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    971\u001b[0m         )\n\u001b[1;32m    972\u001b[0m     ]\n\u001b[0;32m--> 973\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_helper(\n\u001b[1;32m    974\u001b[0m         prompts,\n\u001b[1;32m    975\u001b[0m         stop,\n\u001b[1;32m    976\u001b[0m         run_managers,\n\u001b[1;32m    977\u001b[0m         new_arg_supported\u001b[39m=\u001b[39;49m\u001b[39mbool\u001b[39;49m(new_arg_supported),\n\u001b[1;32m    978\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    979\u001b[0m     )\n\u001b[1;32m    980\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(missing_prompts) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    981\u001b[0m     run_managers \u001b[39m=\u001b[39m [\n\u001b[1;32m    982\u001b[0m         callback_managers[idx]\u001b[39m.\u001b[39mon_llm_start(\n\u001b[1;32m    983\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_serialized,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    990\u001b[0m         \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m missing_prompt_idxs\n\u001b[1;32m    991\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/miniconda3/envs/demo/lib/python3.10/site-packages/langchain_core/language_models/llms.py:792\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_generate_helper\u001b[39m(\n\u001b[1;32m    782\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    783\u001b[0m     prompts: \u001b[39mlist\u001b[39m[\u001b[39mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    789\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    790\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    791\u001b[0m         output \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 792\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[1;32m    793\u001b[0m                 prompts,\n\u001b[1;32m    794\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    795\u001b[0m                 \u001b[39m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    796\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[\u001b[39m0\u001b[39;49m] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    797\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    798\u001b[0m             )\n\u001b[1;32m    799\u001b[0m             \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    800\u001b[0m             \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(prompts, stop\u001b[39m=\u001b[39mstop)\n\u001b[1;32m    801\u001b[0m         )\n\u001b[1;32m    802\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    803\u001b[0m         \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/demo/lib/python3.10/site-packages/langchain_ollama/llms.py:313\u001b[0m, in \u001b[0;36mOllamaLLM._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m generations \u001b[39m=\u001b[39m []\n\u001b[1;32m    312\u001b[0m \u001b[39mfor\u001b[39;00m prompt \u001b[39min\u001b[39;00m prompts:\n\u001b[0;32m--> 313\u001b[0m     final_chunk \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stream_with_aggregation(\n\u001b[1;32m    314\u001b[0m         prompt,\n\u001b[1;32m    315\u001b[0m         stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    316\u001b[0m         run_manager\u001b[39m=\u001b[39;49mrun_manager,\n\u001b[1;32m    317\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m    318\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    319\u001b[0m     )\n\u001b[1;32m    320\u001b[0m     generations\u001b[39m.\u001b[39mappend([final_chunk])\n\u001b[1;32m    321\u001b[0m \u001b[39mreturn\u001b[39;00m LLMResult(generations\u001b[39m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/demo/lib/python3.10/site-packages/langchain_ollama/llms.py:281\u001b[0m, in \u001b[0;36mOllamaLLM._stream_with_aggregation\u001b[0;34m(self, prompt, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_stream_with_aggregation\u001b[39m(\n\u001b[1;32m    273\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    274\u001b[0m     prompt: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    279\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m GenerationChunk:\n\u001b[1;32m    280\u001b[0m     final_chunk \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 281\u001b[0m     \u001b[39mfor\u001b[39;00m stream_resp \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_generate_stream(prompt, stop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    282\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(stream_resp, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    283\u001b[0m             chunk \u001b[39m=\u001b[39m GenerationChunk(\n\u001b[1;32m    284\u001b[0m                 text\u001b[39m=\u001b[39mstream_resp[\u001b[39m\"\u001b[39m\u001b[39mresponse\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mresponse\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m stream_resp \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    285\u001b[0m                 generation_info\u001b[39m=\u001b[39m(\n\u001b[1;32m    286\u001b[0m                     \u001b[39mdict\u001b[39m(stream_resp) \u001b[39mif\u001b[39;00m stream_resp\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mdone\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    287\u001b[0m                 ),\n\u001b[1;32m    288\u001b[0m             )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/demo/lib/python3.10/site-packages/langchain_ollama/llms.py:236\u001b[0m, in \u001b[0;36mOllamaLLM._create_generate_stream\u001b[0;34m(self, prompt, stop, **kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_create_generate_stream\u001b[39m(\n\u001b[1;32m    231\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    232\u001b[0m     prompt: \u001b[39mstr\u001b[39m,\n\u001b[1;32m    233\u001b[0m     stop: Optional[\u001b[39mlist\u001b[39m[\u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    234\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    235\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[Union[Mapping[\u001b[39mstr\u001b[39m, Any], \u001b[39mstr\u001b[39m]]:\n\u001b[0;32m--> 236\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client\u001b[39m.\u001b[39mgenerate(\n\u001b[1;32m    237\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate_params(prompt, stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    238\u001b[0m     )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/demo/lib/python3.10/site-packages/ollama/_client.py:170\u001b[0m, in \u001b[0;36mClient._request.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[39mexcept\u001b[39;00m httpx\u001b[39m.\u001b[39mHTTPStatusError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    169\u001b[0m   e\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mread()\n\u001b[0;32m--> 170\u001b[0m   \u001b[39mraise\u001b[39;00m ResponseError(e\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mtext, e\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mstatus_code) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m r\u001b[39m.\u001b[39miter_lines():\n\u001b[1;32m    173\u001b[0m   part \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(line)\n",
      "\u001b[0;31mResponseError\u001b[0m: model 'llama3.2' not found (status code: 404)"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(template=default_prompt)\n",
    "\n",
    "humanizer_chain = prompt | llm\n",
    "response = humanizer_chain.invoke({\"input_text\": text})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
